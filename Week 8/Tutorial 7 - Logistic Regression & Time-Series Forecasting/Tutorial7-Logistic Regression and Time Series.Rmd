---
title: "Tutorial 7: Logistic Regression and Time Series Forecasting"
author: 'Haowei'
date: 'Due by October 17, 9:00 AM'
output: html_document
---


## Submission Instructions

- Select `output: html_document`.
- Include all code chunks, so include `echo=TRUE` in all chunks.
- Replace the placeholder text, "Type your answer here.", with your own.
- Submit *only* the required question for grading (Part 2: Submission). You can delete everything else for that submission. Remember to include any `library('package_name')` statements that you'll need to run your code and future reproduction. 
- Rename your R Markdown file `T[X]_[MatricNumber].rmd`, and the output will automatically be `T[X]_[MatricNumber].html`. 
    - for example, `T6_12345.html`
    - X is the Tutorial number at the top of this file.
- Submit your both R Markdown file (.rmd) and HTML (.html) to Luminus for tutorial assignments (upload to Luminus under the correct Submission Folder). We shall do the same for practical exam.
- **It is important to be able to code and produce your Rmarkdown output file *independently*.** You are responsible for de-bugging and programming in the practical exam.

## Preparation

```{r load-libraries, echo=FALSE, warning = FALSE, message = FALSE}
# load required packages
# install any package below if it's first time loaded in your computer.
library(dplyr)
library(tidyr)
library(tseries) 
library(TTR) # One alternative for time-series in R
library(forecast) # An alternative for time series in R
library(car) # "Companion to Applied Regression" package, for F-test for linear combination of regression coefs
library(wooldridge) # wooldridge data set will be used in this tutorial
library(ggplot2) # optional. we expect you to know base graphics, but allow ggplot if you find it easier
```
## Part One: Lab Session Completion and Discussion

### Question 1 

- Dataset required: `SGHDBp.csv`

Note: This dataset comes from publically available data from the Singapore Department of Statistics, or SingStat. https://data.gov.sg/dataset/hdb-resale-price-index. 

First, load in the dataset for this question. There is only one variable, which is the average HDB resale price index. Q1 of 2009 is set as the "base" period, and thus has by definition an index value of 100. The index values of the rest of the years are relative to this base value (so a value of 120 means that the average HDB resale price index for that quarter is 120% (or 1.2x) that of the index of Q1/2009).

The code below will also "hold out" Years 2018 and 2019, to test the predictions of our model. This means that we fit the model using all the years except 2018 and 2019, and then once we have the fitted model, we see how well it does on 2018 and 2019.

```{r q1-read-in-dataset, echo=TRUE}
d1_wide = read.csv('SGHDBp.csv', header=T, na.strings = "NA")
# removing unused columns
# This will be the testing values:
d1_wide_HELDOUT <- d1_wide[,114:119] # HOLDING OUT values in 2018 and 2019

# Getting the training values to be fitted into model later:
d1_wide <- d1_wide[,2:113] # keeping values up to and including 2016

# convert to a `ts` object:
d1_ts = ts(unlist(d1_wide[1,1:ncol(d1_wide)], use.names=F), frequency=4, start = c(1990, 1))

# also create a long form data frame. If you are interested in learning more about dplyr, try understanding what each step in this code does by running each line separately (without the last %>%), and inspecting the resulting file using head(d1_long)
d1_long <- d1_wide %>% 
  # gather() converts wide-form to long-form. 
  gather(key="YearQuarter", value="PriceIndex") %>%
  # remove the annoying "X"
  mutate_at("YearQuarter", function(x) {sub(pattern="X", replacement="", x)}) %>% 
  # split "YearQuarter" into a "Year" variable and a "Quarter" variable
  # and make a variable called "TimeIndex" that just goes 1, 2, 3, 4...
  mutate( Year = as.numeric(substr(YearQuarter, start=1, stop=4)),
          Quarter = substr(YearQuarter, start=6, stop=7),
          TimeIndex = 1:length(YearQuarter)) %>%
  # Rearrange the columns in a nicer order
  select("TimeIndex", "YearQuarter", "Year", "Quarter", "PriceIndex")
```

(1a) First, plot the data. There is only one variable, so just plot this against time on the horizontal axis. What do you notice? (Stationary? Trend? Seasons? Cycles?)


```{r q1a-plot, echo=TRUE}
plot(d1_ts, ylab = "Average HDB resale price index")

```

<p style="color:red">
There seems to be an increasing trend. There does not seem to be any apparent seasonal effect.
</p>

(1b) Calculate a Simple Moving Average model to the data, using the Equation we had in class, where 
$m_t = (y_{t} + y_{t-1} + \cdots + y_{t-K+1})/K$

Calculate one with window size of 4 periods (1 year). Calculate a second one of 16 periods (4 years). Plot these two (and the actual data) on the same plot. Discuss what you see.


```{r q1b-sma, echo=TRUE}
# 1 year is 4 periods, n is number of periods
d1_long$SMA4 = TTR::SMA(d1_long$PriceIndex, n = 4)
d1_long$SMA16 = TTR::SMA(d1_long$PriceIndex, n = 16)

# If need to lag
#d1_long$SMA4lagged = dplyr::lag(SMA(d1_long$PriceIndex,n=4),1)
#d1_long$SMA16lagged = dplyr::lag(SMA(d1_long$PriceIndex,n=16),1)


ggplot(d1_long,aes(x=TimeIndex))+
  geom_line(aes(y=PriceIndex,colour='Raw'))+
  geom_line(aes(y=SMA4,colour='Window4'))+
  geom_line(aes(y=SMA16,colour='Window16'))+
  scale_colour_manual(name='Legend',values = c('Raw'='red','Window4'='blue','Window16'='green'))+
  theme_bw()

```

<p style="color:red">
The lines do seem “offset” to the right. This is because moving average models can “follow” recent changes, but they’re always a little slow, and playing catch-up. The peaks happen earlier in the real data.  
<br>
SMA4 lags behind the raw data. SMA16 suppresses random noise(ideosyncrasies) to show the trend. The wider the range , the smoother the curve, When many data points are combined, alot of data is lost, random noise gives us variation. Usually we prefer to retain as much data as possible.  
<br>
From the plot, we can tell that there is no seasonality(Gamma = 0). There is a general upward trend in prices over the years. There is a trend whereby prices will increase and then decrease, before increasing again every 60 time periods(15 years).
</p>

(1c) Based on what you observed about the time-series in Q1a, fit a HoltWinters model to the data. Use the model to predict the next 6 periods (6 quarters), and plot the predictions.


```{r q1c-holtwinters, echo=TRUE}
# alpha beta gamma: check which one is true or false
hw1 = HoltWinters(d1_ts, gamma = FALSE)
hw1

hw1_pred <- predict(hw1, n.ahead =6)
plot(hw1, hw1_pred)

```

<p style="color:red">
Double exponential smoothing, No seasonality(gamma = false). Coefficient a = price index, b = trend.
</p>


(1d) Compare the HoltWinters model's (Q1c) predictions with `d1_wide_HELDOUT`, which contains the actual values for 2018/2019. (Hint: use `XXX[1:Y]` to extract the first few values from the `predict` object). 

What is the mean sum of squared error for these 6 predicted data points? Take the square root of that, which gives the root-mean-squared-error, or RMSE. Report the RMSE.

$RMSE = \sqrt{\frac{1}{n}\sum_i(\hat{y}_i-y_i)^2}$

Make a plot of the Holt-Winters predictions and the actual values in `d1_wide_HELDOUT`, both on the y axis and with time on the horizontal axis. Use colors and/or linetypes to differentiate, and include a legend.

(Bonus: why did I not ask you to use the Simple Moving Average model to forecast the held-out dataset?)


```{r q1d-holtwinters-rmse-plot, echo=TRUE}
#sum_square_errors_hw1 = mean(as.numeric((hw1_pred[1:6] - as.vector(d1_wide_HELDOUT)^2)) # throws error : non-numeric argument to binary operator

# Calculating root mean squared error
sum_squared_errors_hw1 = mean(as.numeric((hw1_pred[1:6] - d1_wide_HELDOUT)^2))
sum_squared_errors_hw1

rmse_hw1 = sqrt(sum_squared_errors_hw1)
rmse_hw1

# To get min max values of graph
plot_min_value = min(c(hw1_pred[1:6], unlist(as.vector(d1_wide_HELDOUT))))
plot_max_value = max(c(hw1_pred[1:6], unlist(as.vector(d1_wide_HELDOUT))))

# Plotting the graph
plot(1:6, hw1_pred[1:6],type='line',ylim = c(plot_min_value,plot_max_value), col='red', xlab='Time', ylab='Price')
lines(1:6, as.vector(d1_wide_HELDOUT),type='l', col='black')
legend('bottomleft',legend=c('Holt-Winters','Actual'),col=c('red','black'),lty=1)

```

<p style="color:red">
No seasonality(gamma) might give error if it does not exist or it is not applicable. We can fit multiple models and select the one with the least amount of error(over/under estimation).
</p>

(1e) For the second-half of this question we shall be using a dataset that's available in R. Load in the dataset using `data(ChickWeight)`. The dataset will then be stored in a variable called `ChickWeight`

There are 4 variables in this long-form dataset, with 578 observations, that comes from a longitudinal experiment in which chicks (baby chickens) were given different types of diets since birth, and the chicks' weights were measured at various time-points. The variables are:

- `weight`. Body weight of the chick at that time point (in grams).
- `Time`. A numeric variable, measuring days since birth at the time of weight measurement. 
- `Chick`. A `factor` that represents a unique Chick. There are in total 50 unique chicks.
- `Diet`. A `factor` with levels 1,2,3,4 that represents the diet that the chicks were fed.

```{r q1e-read-in-data, echo=TRUE}
data(ChickWeight)
head(ChickWeight)
```

First, let's plot some time-series data. Plot the weight vs. time graphs for Chick numbers: `3, 20 and 24`. Put them all on the same graph, make sure each `Chick`'s data is connected by a line, and label each line accordingly.

Which diet did each of these 3 chicks take?


```{r q1e-plot, echo=TRUE}
subset2a <- subset(ChickWeight, ChickWeight$Chick %in% c('3', '20', '24'))

ggplot(subset2a, aes(x=Time, y=weight, colour=Chick)) + geom_line() + theme_bw()
```
<p style="color:red">
Chick 3 and chick 20 were fed on diet 1, chick 24 was fed on diet 2.
</p>


(1f) From (Q1e), you can already see that (i) there is a lot of individual variation between chicks with the same diet, and (ii) there may be differences across diets. Of course, we just took three chicks to plot, so we can't draw any conclusions just like that. 

Make a subset of all the chicks that took `Diet 3`. For this subset, fit a linear model predicting `weight` just using `Time`. This is a regression-based time-series model where our predictor, our "X", is just an index now that represents time. Interpret the intercept and slope coefficients.

(Challenge: Try to plot all the `Chick`s on this diet on the same plot. Make sure each line corresponds to one `Chick`.)

```{r q1f-model, echo=TRUE}
subset2b <- ChickWeight %>% filter(Diet == 3)
summary(lm(weight ~ Time,subset2b))

ggplot(subset2b, aes(x=Time, y=weight, group=Chick, color=Chick)) + geom_line() + theme_bw()
```

<p style="color:red">
120 samples(118 df+2 variables). Coefficient of intercept of 18.250 is the mean weight of chick at birth. Coefficient of time is 11.423. With every day that passes, we would expect an average of 11.423g increase in weight for a chick on diet 3. Since the coefficient of time = 2e-16<0.05, we can conclude that at the 5% level of significance, the coefficient of time is statistically significant.
</p>

(1g) Now let's look at two groups. Make a subset of `Chicks` who are on `Diet 3` and `Diet 1`. Make a dummy variable to indicate which `Diet` they are on. (To give you some practice in manipulating variables, let's say that 3 is the reference group, and this dummy variable should be 1 if the Chick is on Diet 1 and 0 if the Chick is on Diet 3).

If I'm interested in seeing whether the `Diet` affects `Chicks`' growth, what is the linear regression model I should test? Run this model, and interpret the coefficients on the variables in the model. Which of the two `Diet`s seem to be better for growth?

(Challenge: Try to plot all the Chicks in this analysis on the same plot. Similar to Q1f, make sure each Chick corresponds to one line. But for this graph, let's color the lines by Diet. This may help you to visualize the coefficients you see in the model.)


```{r q1g-model, echo=TRUE}
subset2c <- ChickWeight %>% filter(Diet == 3 | Diet == 1) %>% 
  mutate(Dummy = factor(Diet, levels=c("3","1"), labels=c("3","1")))

summary(lm(weight ~ Time*Dummy, subset2c))

ggplot(subset2c,aes(x=Time,y=weight,group=Chick,colour=Diet))+geom_line()+theme_bw()
```
###Explaining using one example
<p style = "color:blue">
Weight = B0 + B1T + B2D1 + ....  
Since reference group is 3: 
Intercept: At birth, chick on diet 3 is 18.25g  
Time: Every day that passes, chick on diet 3 +11.4g  
Dummy1: Is factor variable so only (0):(if on diet 3) or (1):(if on diet 1). If diet 1, at birth will be 18.25 + 12.68  
Time:Dummy1 : Every day that passes, diet 1 chick increase by 11.43 - 4.58g  
</p>

###Answering 1f
<p style="color:red">
Coefficient of intercept is 18.250, meaning that the mean weight of chick on diet 3 at birth is 18.25g. Since the p-value < 0.05, we conclude that at the 5% level of significance, the mean weight of chicks on diet 3 at birth is statistically different from 0.  
<br>
Coefficient of time is 11.423, meaning that on average, chicks on diet 3 put on 11.43g per day. Since the p-value < 0.05, we conclude that at the 5% level of significance, there the mean increase in weight of chicks on diet 1 per day is statistically different from 0.  
<br>
Coefficient on Dummy1 is 12.68, meaning that on average, at birth chicks on diet 1 are 12.68g heavier than the chicks on diet 3. Since the p-value of 0.08880>0.05, we conclude that at the 5% level of significance, there is statistically no true difference between the mean weight of chicks on diet 3 and diet 1 at birth.  
<br>
Coefficient on TimeDummy is -4.58. This means that on average,chicks on diet 1 put on 4.5811g less weight per day than chicks on diet 3. Chicks on diet 1 put on (11.4229-4.5811)=6.8418g of weight per day. Since p-value of Time:Dummy is 6.1e-14<0.05, we conclude that at the 5% level of significance, the difference between the coefficient of Time:Dummy and Time is statistically significant, meaning that the growth of chicks on diet 1 is less than the chicks on diet 3.
</p>

(1h) Finally, let's look at all four `Diet`s. This last part is building up from the previous sub-parts. You already know enough to do this, you just have to be very careful in interpreting each step.

Now, let's use the full dataset. `ChickWeight$Diet` is already a factor, so let's just use `Diet` as the moderator, and see if `Diet` moderates growth rates. You should be running the same `lm()` model 

What is the reference group of: `ChickWeight$Diet`?

How do you interpret each interaction coefficients?


Which seems to be the best `Diet` for growth (i.e., with the highest growth rate)? Which seems to be the worst?


(challenge points for plotting all these results. One way I would recommend visualizing them is putting all the `Chick`s in one `Diet` on one graph, and have four graphs side-by-side. If you use ggplot it's called facet-ing. Your tutor will show this graph in class using `ggplot`, but I will leave this as a bonus challenge for you.)


```{r q1h-model, echo=TRUE}
summary(lm(weight~Time*Diet, ChickWeight))
ggplot(ChickWeight,aes(x=Time,y=weight,group=Chick,colour=Chick))+geom_line()+facet_grid(~Diet)+theme_bw()+theme(legend.position = 'None')

```

<p style="color:red">
Intercept:30.93. This means that the average weight of a chick on diet 1 at birth is 30.93g. Since the p-value < 0.05, we conclude that at the 5% level of significance, the mean weight of chicks on diet 1 at birth is statistically different from 0.  
<br>
Time: 6.84. This means that on average, chicks on diet 1 put on 6.84g per day. Since the p-value < 0.05, we conclude that at the 5% level of significance, there the mean increase in weight of chicks on diet 1 per day is statistically different from 0.  
<br>
Diet 2: -2.29. This means that on average, at birth chicks on diet 2 are 30.93 - 2.29 = 28.64g, which is 2.29g lighter than chicks on diet 1. Since the p-value > 0.05, we conclude that at the 5% level of significance, there is statistically no difference between the mean weight of chicks on diet 1 at birth and the mean weight of chicks on diet 2 at birth.  
<br>
Diet 3: -12.68. This means that on average, at birth chicks on diet 3 are 30.93 - 12.68 = 18.25g, which is 12.68g lighter than chicks on diet 1. Since the p-value > 0.05, we conclude that at the 5% level of significance, there is statistically no difference between the mean weight of chicks on diet 1 at birth and the mean weight of chicks on diet 3 at birth.  
<br>
Diet 4: -0.1389. This means that on average, at birth chicks on diet 4 are 30.93 - 0.13 = 30.8g, which is 0.13g lighter than chicks on diet 1. Since the p-value > 0.05, we conclude that at the 5% level of significance, there is statistically no difference between the mean weight of chicks on diet 1 at birth and the mean weight of chicks on diet 4 at birth.  
<br>
Time:Diet2: 1.7673. This means that on average, chicks on diet 2 put on 6.84 + 1.76 = 8.6g per day, which is 1.76g more than the weight which chicks on diet 1 put on per per day. Since the p-value < 0.05, we conclude that at the 5% level of significance, the true difference between the increase in weight of chicks on diet 1 per day and the increase in weight of chicks on diet 2 per day is not 0.  
<br>
Time:Diet3 4.5811. This means that on average, chicks on diet 3 put on 6.84 + 4.58 = 11.42g per day, which is 4.58g more than the weight which chicks on diet 1 put on per per day. Since the p-value < 0.05, we conclude that at the 5% level of significance, the true difference between the increase in weight of chicks on diet 1 per day and the increase in weight of chicks on diet 3 per day is not 0.  
<br>
Time:Diet4 2.8726. This means that on average, chicks on diet 4 put on 6.84 + 2.87 = 9.41g per day, which is 2.87g more than the weight which chicks on diet 1 put on per per day. Since the p-value < 0.05, we conclude that at the 5% level of significance, the true difference between the increase in weight of chicks on diet 1 per day and the increase in weight of chicks on diet 4 per day is not 0.  
</p>

## Part Two: Assignment Submission (25 points)

### Question 3 (total 17 points)

We will study fertility rates with two time-series datasets on U.S. and Singapore, respectively. 

High fertility rate is essential for long-term growth in any economy. Many countries are troubled with low or even negative fertility rate. For example, the fertility rate in Singapore in general shows a decreasing trend in recent decades (https://www.channelnewsasia.com/news/singapore/number-of-babies-born-in-singapore-falls-to-lowest-in-8-years-11743722). Fertility is affected by many socio-economic factors, including single rate, family disposable income, level of tax duty, war attrition, contraception technology, etc. 

Let's first take a look at the fertility rate in the United States in 20th century. `fertil3` data contains information about woman's fertility rate and personal tax exemption in U.S. in early-mid 1900s. 

- Dataset required: `data('fertil3')` in `wooldridge` package.

(Note: This dataset comes from a publicly available dataset from Jeffery Wooldridge Textbook. See data description in https://rdrr.io/cran/wooldridge/man/fertil3.html)

First, load in the time series data for this question. There are 72 observations on 24 variables about women fertility rate between year 1913 and 1984. Key variables are listed below:

- `gfr`: births per 1000 women between age 15 and 44.
- `pe`: real value personal tax exemption in US dollars.
- `pe_1`: 1-period lag of `pe`, real value personal tax exemption in US dollars from last year.
- `pe_2`: 2-period lag of `pe`, real value personal tax exemption in US dollars from two years ago.
- `ww2`: a binary variable = 1 during World War 2 between 1941 and 1945.
- `pill`: a binary variable = 1 from 1963 on when the birth control pill was made available for contraception.

```{r q3-read-dataset, echo=TRUE}
# read dataset into workplace, note that you need library(wooldridge) to load this data set
data('fertil3')
# if you want, is can be converted to ts object
fertil = ts(fertil3, frequency = 1, start = 1913)
```

Q3(a) Start off by plotting `gfr` (fertility rate) and `pe_1` (personal tax exemption from last year) against time in the same plot. What do you observe from the time series plots alone: do you see any trend or seasonality? Do you think that `gfr` time series is stationary? (2 points)

Remark: You should be able to visualize data with R base graphics at least. Other alternative R graphic packages such as `ggplot` are welcomed additions.

```{r q3a-plot, echo=TRUE}
ggplot(fertil3,aes(x=year))+
  ylab("Value") +
  geom_line(aes(y=gfr,colour='gfr'))+
  geom_line(aes(y=pe_1,colour='pe_1'))+
  scale_colour_manual(name='Legend',values = c('gfr'='red','pe_1'='blue'))+
  theme_bw()

# plot.ts(fertil[,'gfr'])
# plot.ts(fertil[,'pe'])
# ts.plot(fertil[,'gfr'], fertil[,'pe'], 
         #gpars = list(xlab = 'Year', ylab = 'Value', col = c('darkred','darkblue')))
# legend("topright", legend = c('gfr', 'pe'), col = c('darkred','darkblue'), lty = 1)

adf.test(fertil[,"gfr"])

```

<p style="color:red">
Both `gfr` and `pe` exhibit long-term trend. Between 1913 and 1938, gfr had a decreasing trend while pe had a increasing trend and from 1938 on, gfr and pe moved in the same direction with pe being more volatile. 
`gfr` has a nonlinear trend and shows sign of autocorrelation, thus likely non-stationary.  
Using Augmented Dickey-Fuller Test for unit-root of `gfr` : The large p-value of ADF test shows little evidence that we can reject the null hypothesis, i.e. there exists a unit root for time series `gfr`.
</p>


Q3(b) Many labor economists believe that fertility rate is affected by the economic policy such as personal tax exemption, e.g., `pe` and their lag terms. Run a linear regression of `gfr` on `pe`, `pe_1`, and `pe_2`. Interpret the coefficient before `pe`. Is it statistically significant? What's your conclusion about whether tax exemption improves fertility rate, from the regression output? (3 points)

```{r q3b, echo=TRUE}
fit_1 <- lm(gfr ~ pe + pe_1 + pe_2, fertil3)
summary(fit_1)

```

<p style="color:red">
Coefficient of `pe` = -0.01584. Every unit increase of real value personal tax exemption in US dollars decreases the births per 1000 women between age 15 and 44 by 0.01584, on average, given all other predictors constant.  
However, the p-value=0.910 > 0.05, thus we do not have sufficient evidence to reject the null hypothesis that the coefficient is 0. Therefore, the coefficient is statistically insignificant at the 5% level of confidence.  
From the regression output, it seems that tax exemption does not improve fertility rate as there is no relationship between fertility rate and tax exemption.
</p>


Q3(c) Now, include a time trend variable `t` into the regression model in part (b). Interpret the coefficients for `pe_2` and `t`. What's been changed in the regression result compared to previous one, in terms of coefficients (its sign and magnitude), goodness-of-fit R-square and F-test of the linear regression model? Which model do you think we should choose, and why? (4 points)

```{r q3c, echo=TRUE}
fit_2 <- lm(gfr ~ pe + pe_1 + pe_2 + t, fertil3)
summary(fit_2)

```

<p style="color:red">
Coefficient of `pe_2` = 0.309055. Every unit increase of real value personal tax exemption in US dollars two years ago increases the births per 1000 women between age 15 and 44 by 0.309055,on average, given all other predictors constant.  
The p-value=0.000948 is very small,(< 0.05), thus we have sufficient evidence to reject the null hypothesis that the intercept is 0 and conclude that the slope parameter is statistically significant at the 5% level of confidence.  
<br>
Coefficient of `t` = -1.068292. This means that holding all other constant, as a long-term trend, the births per 1000 women between age 15 and 44 decreases by 1.068292 on average *every year*.
The p-value=2.6e-15 is very small,(< 0.05), thus we have sufficient evidence to reject the null hypothesis that the intercept is 0 and conclude that the slope parameter is statistically significant at the 5% level of confidence.  
<br>
From the regression output, we can see both slope parameters of `pe_2` and time trend variable `t` are statistically significant from zero while slope of `pe_2` was insignificant in the previous simple linear model without time trend variable `t`.  
The sign of coefficient of `pe_2` is not only reversed but its magnitude increases from 0.05390 to 0.309055. In addition, R square (goodness-of-fit) increases from 0.006135 to 0.6229 with inclusion of time trend variable t. The model becomes statistically significant due to large F-statistic in F-test.  
<br>
Since both `gfr` and `pe_2` shows long-term trend and thus non-stationary. Adding a time trend variable t in the linear regression model helps to de-trend these two time series and deal with non-stationary issue. Between these two models, the one with trend variable t is preferred: increasing personal tax exemption might likely improves fertility rate.
</p>


As we have discussed fertility rate in U.S. between 1913 and 1984 in previous question, now let's turn our attention to fertility rate in Singapore in modern times, where the fertility rate plummets to lowest figure in 2020 in history. 

- Dataset required: `SGfertil20.csv`

Note: This dataset comes from publicly available data from the Singapore Department of Statistics, or SingStat. https://www.singstat.gov.sg/find-data/search-by-theme/population/births-and-fertility/latest-data. You can download the data yourself and explore. For homework, I have done some data cleaning as to have only one time series variable ,`gfr`, in `sgfertil.csv` which contains information on fertility rate of childbearing age women between age 15 and 44 in Singapore comparable to that in U.S. 

```{r q3-fertil-data-input, echo=TRUE}
# read raw data from 'sgfertil.csv'
sgfertil = read.csv(file = 'SGfertil20.csv', header= TRUE)
colnames(sgfertil)[1]='Data.Series'

sgfertil = sgfertil %>% 
  # given our focus in univariate time-series analysis on total fertility rate..
  select(Data.Series, Total.Fertility.Rate..Per.Female.) %>%
  # rename
  rename(year = Data.Series, gfr = Total.Fertility.Rate..Per.Female.) %>%
  # sort the data bt ascending on year
  arrange(year)
# extract the key time series of our interest and convert it to ts obj
gfrsg_ts = ts(sgfertil$gfr, start = 1960, end = 2020, frequency = 1)
```
`sgfertil` contains yearly data about gross fertility rate per thousand women between 1960 and 2020. A quick description of the data:

Obs:  61
- `gfr`: total fertility rate of women in their childbearing age between 15 and 44, #births per thousand
- `year`: as described.

Q3(d) First plot the `gfrsg` against time. Draw the time series plot of `gfrsg`. Describe what you observe from the plot: do you see any trend or/and seasonality? Do you think total fertility rate time series in Singapore `gfrsg` stationary? (2 points)

```{r q3d, echo=TRUE}
plot.ts(gfrsg_ts, col = 'darkred')

```

<p style="color:red">
Total fertility rate in Singapore clearly shows a decreasing long-term trend. There seems to be no obvious seasonality in the time series data of `gfrsg`. With the decreasing trend, `gfrsg` is unlikely to be stationary.
</p>


Q3(e) In order to predict future fertility rate using information contained in the time series of `gfrsg` itself, let's make forecast by simply fitting a moving average series. First, compute the moving averages with window length of 4 years and plot both the original and moving average series in the same graph. Second, obtain the moving average prediction series. What is the MA predicted value for Year 2021? (3 points)

```{r q3e, echo=TRUE}
# Take note n is 4 here because each year is one unit, compared to Lab7 where each time period was 1/4 of a year
sgfertil$SMA4 = TTR::SMA(sgfertil$gfr, n=4)


ggplot(sgfertil,aes(x=year))+
  geom_line(aes(y=gfr,colour='gfr'))+
  geom_line(aes(y=SMA4,colour='SMA4'))+
  scale_colour_manual(name='Legend',values = c('gfr'='red','SMA4'='blue'))+
  theme_bw()

# Obtaining moving average prediction Series using window length of 4 years
# Window4 = 1/4 (Yt + Yt−1 + Yt-2 + Yt−3)

sgfertil$SMA4_predict = dplyr::lag(sgfertil$SMA4,1) # moving average prediction series
sgfertil$SMA4_predict
# MA Predicted value for Year 2021
sgfertil %>% filter(year == 2020) %>% select(SMA4)
```

<p style="color:red">
The MA predicted value for Year 2021 is 1.135.  
</p>


Q3(f) Based on your observation in part (d), predict the fertility rates for Year 2021 and Year 2024 with Holt-Winter model of your choice. (3 points)

```{r q3f, echo=TRUE}

hw1 = HoltWinters(gfrsg_ts, gamma = FALSE)
hw1

# prediction model
hw1_pred <- predict(hw1, n.ahead = 4)

# Predicting fertility rates for Year 2021 using indexing
hw1_pred[1]

# Predicting fertility rates for Year 2024 using indexing
hw1_pred[4]
```

<p style="color:red">
The predicted fertility rate for Year 2021 is 1.08067.  
The predicted fertility rate for Year 2024 is 1.022681.  
</p>


### Question 4 (Total 8 points)

- Dataset required: `data('recid')`

Recidivism rate in Singapore is 24% in 2016 (https://data.gov.sg/dataset/recidivism-rate). Criminals tend to relapse into criminal offense after the release from the prison. Recidivism is costly and causes serious social and economical problem. It is not only wasteful with the resources invested in prison, including staffing, infrastructure investment, daily operation cost, and economic opportunity cost for both prisoners and staffs (i.e., labor values that could be generated elsewhere other than being locked up and guarding in prison, respectively), but also harms the community for the second time due to crime re-commitment. Recidivism is thus a critical evaluating metric for prison performance, e.g., rehabilitation or training program. `recid.csv` contains 1445 observations of recidivism cases in United States where it currently has the largest prison population in the world (about one out every five peope imprisoned in the world is incarcerated in the United States).

```{r q3-data-input, echo=TRUE}
recid = read.csv(file = 'recid.csv', header= TRUE)
```

C.-F. Chung, P. Schmidt, and A.D. Witte (1991), “Survival Analysis: A Survey,” Journal of Quantitative Criminology 7, 59-98.
Data Source: https://www.cengage.com/cgi-wadsworth/course_products_wp.pl?fid=M20b&product_isbn_issn=9781111531041. A quick description of the data:

Obs:  1445

  1. black                    =1 if black
  2. alcohol                  =1 if alcohol problems
  3. drugs                    =1 if drug history
  4. super                    =1 if release supervised
  5. married                  =1 if married when incarc.
  6. felon                    =1 if felony sentence
  7. workprg                  =1 if in N.C. pris. work prg.
  8. property                 =1 if property crime
  9. person                   =1 if crime against person
 10. priors                   # prior convictions
 11. educ                     years of schooling
 12. rules                    # rules violations in prison
 13. age                      in months
 14. tserved                  time served, rounded to months
 15. follow                   length follow period, months
 16. durat                    max(time until return, follow) in month
 17. cens                     =1 if duration right censored
 18. ldurat                   log(durat)

Q4(a) Criminal offense lawyers usually fight for supervised release of the offender. Given five variables in `recid` data set: `super`, `rules`, `age`, `tserved`, `married`, `prior` and `black`, how could you help the law firm to predict the outcome of a recent case given that a 55-year-old married black client has been serving the jail time for 5 years and 11 months with no prior conviction and during which broken no rule in the prison? 
In order to evaluate the likelihood to answer such question. First of all, write down the regression model you propose to use. Clearly define or label your variables.  (2 point)

```{r q4a, echo=TRUE}

# Proposed regression model
# Use GLM with Binomial Family as dependent variable is categorical (Binary dependent variable)
fit_supervisedRelease <- glm(super ~ rules+age+tserved+married+priors+black, family = binomial ,data=recid)

```

<p style="color:red">
For this question, we are interested in predicting the outcome of a binary dependent variable (i.e., Released Supervised or Not Released Supervised). Hence, we can conduct a logistic regression. Our logistic regression model is: logit(p) = log (p/(1-p)) = beta0 + beta1`rules` + beta2`age` + beta3`tserved` + beta4`married` + beta5`priors` + beta6`black`, where p is the probability of an offender being released supervised.  
<b> NOTE: for Log regression, it is BETA not b </b>
</p>


Q4(b) Run your regression model in R and interpret the coefficients before `priors` and `black`. Based on the results, do you think there was any racial discrimination in terms of likelihood of supervised release, and why? (3 point)

```{r q4b, echo=TRUE}
summary(fit_supervisedRelease)
```

<p style="color:red">
Coefficient of `priors` = -0.0943194. For every unit increase in number of prior convictions, the log-odds for supervised release of the offender decreases by 0.0943194 given all other predictors constant.  
Moreover, The p-value=7.14e-05(< 0.05), thus we can reject the null hypothesis that this slope parameter is = 0, i.e., the coefficient is significantly different from zero.  
<br>
Coefficient of `black` = 0.1114900. If the offender is black, the log-odds for supervised release of the offender increases by 0.1114900 given all other predictors constant.   
However, p-value=0.352001(> 0.05), we cannot reject the null hypothesis that this slope parameter is = 0, i.e., the coefficient is not significantly different from zero.  
<br>
According to the model, there was no racial discrimination in terms of likelihood of supervised release as from the regression output, it seems that there is no relationship between being black and the likelihood of supervised release.
(Since the two-sided z-test for marginal effect for black fails to reject its null. There is no evidence for racial discrimination regarding the chance for supervised release.)
</p>


Q4(c) Finally, what is the predicted probability for supervised release for the new client? (2 point)

```{r q4c, echo=TRUE}
newClient = data.frame(rules = 0, age = 55*12, tserved = 71, married = 1, priors = 0, black = 1)
# Evaluating likelihood
# use type = 'response'
predict(fit_supervisedRelease, newdata = newClient, type = 'response')
```

<p style="color:red">
The predicted probability for supervised release of a 55-year-old married black client has been serving the jail time for 5 years and 11 months with no prior conviction and during which broken no rule in the prison is 0.9616305.
</p>


Q4(d) Successfully debug the Rmarkdown file and produce an HTML for submission. (1 point)


