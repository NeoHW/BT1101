---
title: "1718 SEM2"
author: "BT1101 Student. Neo Haowei"
date: '25 Nov 2021, 1:00 - 3:30 PM'
output: html_document
---

## Question List
### - Q1 : Test statistics, Hypo testing
### - Q2 : Definitions
### - Q3 : Descriptive Analytics
### - Q4 : Linear Optimization
### - Q5 : Linear Optimization (Matrix style) , 5b??
### - Q6 : (6a)SMA, Mean Square Error ; (6b) exponential smoothing table using RMSE

To check: q1b, 5b, 6

## Preparation

```{r preparation, echo=TRUE, warning = FALSE, message = FALSE}

library('libcoin')
library("rcompanion") #this package is required for transformTukey function
library("rstatix")
library("Rmisc") 
library("dplyr") #need to call the library before you use the package
library("tidyr")
library("rpivotTable")
library("knitr")
library("psych")
library(tidyverse)
library(ggplot2) 
library(TTR)
library(factoextra) # for fviz_cluster()
library(lpSolve)

# predictive/prescriptive analytics
library(tseries) 
library(forecast)
library(lpSolve)

# descriptive analytics
library(psych)
library(Rmisc)
library(rcompanion)
library(rpivotTable)
library(EnvStats) 
library(car)
library(rstatix)

# general use
library(dplyr)
library(tidyr)
library(knitr)
```

## Some Formulas:
<p style="color:blue">
Mean use T-test, Proportion Z-test  
test statistic z for proportion: (proportion-probability) / sqrt(probability * (1-probability)/n)  
test statistic t for comparing means: (new mean-old mean)/(sd/sqrt(n))  
z critical value: qnorm(0.05) # 5% level of significance, one tailed  
t critical value: qt(0.975,df=n-1)  
confidence interval for mean: mean +- `test statistic t` * (sd/sqrt(n)) OR use CI()  
prediction interval: mean +- `test statistic t` * (sd * sqrt(1+(1/n))  
confidence interval for proportions: p +- `test statistic z`  * (sqrt(p*(1-p)/n)) 
</p>


### Question 1a
```{r q1a}

# H0 : proportion >= 0.7
# H1 : proportion < 0.7

# test statistic z for proportion: (proportion-probability) / sqrt(probability * (1-probability)/n)
proportionRiskAverse <- 20/32 #proportion
z <- (proportionRiskAverse-0.7) / sqrt(0.7*(1-0.7)/32)
z

# Critical Value
cv95 <- qnorm(0.05)
cv95

z<cv95

```

H0: Proportion >= 0.70  
H1: Proportion < 0.70  
Since we are conducting a lower one-tailed test and the z value is not lesser than the critical value, we do not have sufficient evidence to reject H0 as the z value does not fall within the rejection region. We do not reject the conclusion that the proportion of investors who are risk-averse is at least 0.7  


### Question 1b
```{r q1b}
# test statistic t for comparing means: (new mean-old mean)/(sd/sqrt(n))
tstat <- (365-350)/(38/sqrt(100))
tstat

lcv <- qnorm(0.05/2)
lcv
ucv <- -qnorm(0.05/2)
ucv

tstat < ucv 
```

H0: New mean <= 350  
H1: New mean > 350  
The t-statistic is -3.947368. From our results, the t-statistics is more than the critical value, we have sufficient evidence to reject H0, at 5% level of significance. Therefore, this data provides statistical evidence that the quality of recent applicants have improved such that the mean score is greater than 350.  


### Question 2a & 2b
Reliability : data is accurate and consistent  
Validity : data correctly measures what it is supposed to measure  
<br>
I agree with the above statement as there must be quality in the data collected. A failure in any of the 3 (completeness, reliability and validity) will lead to incorrect data analysis.

### Question 3

**Q3a**  
Gender : M if Male, F is female  
Body Type: Type of body the persons has  
BMI classification: Classifying the persons weight class according to their BMI  
Time Spent in Gym: Time spent in the gym    
Pant Size(inches): Pant size in inches  
Weight : Weight of the person  
Height(Inches) : Height of person in inches  
Weight Lift(Days) : Number of days person weight lifts  
Lifting Sessions(Mins) : How long each lifting session lasts  
Running Times(Hours) : Time spent on running in hours  
Distance of Run(Miles) : Distance ran in miles
<br>
**Q3b**  
The distribution of `BMI Calculation` is normal. According to the Shapiro-Wilk normality test, w value = 0.95144 which is close to 1. Moreover, p-value = 0.3133(>0.05), the probability of the available data is more than 5% given that the null hypothesis is true. We do not reject H0. There is evidence that data is normally distributed.  
With reference to the histogram as well, we can see that the normal distribution best describes the variable 'BMI calculation' due to its bell-shape.
<br>
**Q3bii**  
According to figure 3.5, there are 10 females.  
<br>
**Q3biii**  
Mean BMI calculation for members with "average" body type is (22.56 * 4 + 27.97 * 4)/8 = 25.27  
<br>
**Q3biv**  
``` {r q3biv Confidence Interval, echo = T}
uCIage95t <- 162.29 - qt(0.025, df=23-1)*107.80/sqrt(23) # df = number of sample values - number of estimate parameters
lCIage95t <- 162.29 + qt(0.025, df=23-1)*107.80/sqrt(23)
print(cbind(lCIage95t, uCIage95t), digits=5)
```

The 95% prediction interval for `time spent in gym` is [115.67,208.91]. Given the time spent in the gym, the time a person spent in the gym will lie within this interval with a 95% level of confidence. With repeated sampling, 95% of such constructed predictive intervals would contain the new time spent in gym.  
<br>
**Q3d**  
Relationship between BMI Calculation and   
Time Spent in Gym: 0.06 (p-value: 1)  
Pant Size: 0.89 (p-value: 0)  
weight: 0.93 (p-value: 0)  
height: 0.49 (p-value: 0.49)   
Weight Lift(days): 0.03 (p-value: 1)  
Lifting Sessions(Mins): 0.23 (p-value: 1)  
Running times : -0.23 (p-value: 1)  
Distance of Run : -0.42 (p-value: 1)  
<br>
BMI calculation has a strong positive linear relationship with Pant Size and Weight, moderate positive linear relationship with Height and weak positive linear relationship with Lifting Session. There is almost no linear relationship between BMI calculation with Weight Lift and Time spent in gym. BMI calculation also has a moderate negative linear relationship with distance of run, and a weak negative linear relationship with running times.
<br>
If p-value < 0.05, the correlation is significant at the 5% level of significance. Only Pant Size and Weight is correlated to BMI calculation at the 5% level of significance.  
<br>
**Q3ei**  
3.8:  
H0: mean of `BMI calculation` for males = mean of `BMI calculation` for females.  
H1: mean of `BMI calculation` for males =/= mean of `BMI calculation` for females.  
3.9:  
H0: mean of `Time spent in gym` for males = mean of `Time spent in gym` for females.  
H1: mean of `Time spent in gym` for males =/= mean of `Time spent in gym` for females.  
<br>
**Q3eii**  
From 3.8:  
Based on the results (t=-3.0439 & p-value=0.006188(<0.05)), we have sufficient evidence to reject H0 and can accept that the population mean BMI for males is significantly different from the population mean BMI for females at the 5% level of significance.  
<br>
From 3.9:  
Based on the results (t=-1.9416 & p-value=0.06628(>0.05)), we have insufficient evidence to reject H0. We can conclude that the mean time spent in gym for males is the same as the time spent in the gym for females at the 5% level of significance.  
<br>
**Q3f**  
He could use welch-anova test for comparison as there are more than 2 body types.  
H0: mean time spent in the gym is the same for all 5 body types  
H1: mean time spent in the gym for at least 1 body type is different from the others  
If the p-value<0.05, there is sufficient evidence to reject H0 in favour of H1 at the 5% level of significance and conclude that the mean time spent in gym for at least one body type is different from the other body types. If p-value<0.05, we would perform post-hoc tests to figure out which pair of body types have a difference in mean time spent in gym. 
An example is games howell test which is used to determine which pair of means is significantly different, which happens if p-value < 0.05.  
<br>
Sample r code:  
wa.out1 <- df2 %>% welch_anova_test(`Time Spent in Gym` ~ `Body Type`)  
gh.out1 <- games_howell_test(df2, `Time Spent in Gym` ~ `Body Type`)  
wa.out1  
gh.out1  
<br>
**Q3g**  
``` {r q3g Prediction Interval, echo = T}
uPI.age <- 2.07 - (qt(0.025, df = (23-1))*1.54*sqrt(1+1/23)) # calculating for 95% interval     # 0.005/2 = 0.025 in the formula
lPI.age <- 2.07 + (qt(0.025, df = (23-1))*1.54*sqrt(1+1/23))
cbind(lPI.age, uPI.age)

## do we need to change lower interval to 0?
```
Since the value for distance of run cannot be negative, as you cannot run a negative distance, the lowerPI value lower limit would be limited to 0.  
The 95% prediction interval for sales amount of a new DVD transaction is [0,5.332455]. Given the observed DVD prices, the price of a new DVD will lie within this interval with a 95% level of confidence. With repeated sampling, 95% of such constructed predictive intervals would contain the new DVD price.

### Question 4: Optimization model to minimize costs.

Minimize total cost using decision variables $X_1$, $X_2$, $X_3$, $X_4$, $X_5$, $X_6$, $X_7$, $X_8$, $X_9$| Cost = 0.48$X_1$ + 0.39$X_2$ + 0.21$X_3$ +  0.23$X_4$ + 0.15$X_5$ +  0.11$X_6$ + 0.66$X_7$ + 0.25$X_8$ + 0.52$X_9$
--- | --- 
Subject to |  
Protein Constraint | 15.7$X_1$ + 13.1$X_2$ + 8.6$X_3$ +  17.2$X_4$ + 6.7$X_5$ +  12.4$X_6$ + 17.4$X_7$ + 13.9$X_8$ + 15.3$X_9$ $\geq$ 11
Fat Constraint | 28.8$X_1$ + 4.3$X_2$ + 4.7$X_3$ + 6.2$X_4$ + 3.9$X_5$ +  1.5$X_6$ + 14.7$X_7$ + 4.6$X_8$ + 12.4$X_9$ $\geq$ 28
Fiber Constraint | 25.7$X_1$ + 5.6$X_2$ + 2.8$X_3$ +  3.1$X_4$ + 1.5$X_5$ +  2.5$X_6$ + 23.4$X_7$ + 13.2$X_8$ + 15.3$X_9$ $\leq$ 4
Non-Negativity Constraints | $X_1$ to $X_9$ all $\geq$ 0

```{r q4-lp, echo=TRUE}

# defining parameters 
objective.fn = c(0.48,0.39,0.21,0.23,0.15,0.11,0.66,0.25,0.52)
const.mat = matrix(c(15.7,13.1,8.6,17.2,6.7,12.4,17.4,13.9,15.3,
                     28.8,4.3,4.7,6.2,3.9,1.5,14.7,4.6,12.4,
                     25.7,5.6,2.8,3.1,1.5,2.5,23.4,13.2,15.3), ncol = 9, byrow = TRUE)
const.dir = c(rep('>=', 2), '<=')
const.rhs = c(11,28,4)

# solving the linear problem
lp.solution = lp(direction = 'min', objective.fn, 
              const.mat, const.dir, const.rhs,
              compute.sens = TRUE)
print(lp.solution$solution)
print(lp.solution)


```

From the model, we are unable to find a combination of ingredients that are able to fulfill the criteria of having at least 11% protein, 28% fat, and no more than 4% of fiber. Hence, there are no feasible solutions.

### Question 5a: Optimization model to minimize transportation costs.
Minimize costs using decision variables $X_{11}$, $X_{12}$, $X_{13}$, $X_{14}$, $X_{15}$, $X_{21}$, $X_{22}$, $X_{23}$, $X_{24}$, $X_{25}$, $X_{31}$, $X_{32}$, $X_{33}$, $X_{34}$, $X_{35}$ | Costs= 12.40$X_{11}$, 13.75$X_{12}$, 12.30$X_{13}$, 12.15$X_{14}$, 18.35$X_{15}$, 8.5$X_{21}$, 18.55$X_{22}$, 8.80$X_{23}$, 9.85$X_{24}$, 16.45$X_{25}$, 9.3$X_{31}$, 14.25$X_{32}$, 6.45$X_{33}$, 11.15$X_{34}$, 14.95$X_{35}$
--- | --- 
Subject to |  
Maximum for factory A | $X_{11}$ + $X_{12}$ + $X_{13}$ + $X_{14}$ + $X_{15}$ $\leq$ 1300
Maximum for factory B | $X_{21}$ + $X_{22}$ + $X_{23}$ + $X_{24}$ + $X_{25}$ $\leq$ 900
Maximum for factory C | $X_{31}$ + $X_{32}$ + $X_{33}$ + $X_{34}$ + $X_{35}$ $\leq$ 400
Demand for Center A | $X_{11}$ + $X_{21}$ + $X_{31}$ $\geq$ 160  
Demand for Center B | $X_{12}$ + $X_{22}$ + $X_{32}$ $\geq$ 130
Demand for Center C | $X_{13}$ + $X_{23}$ + $X_{33}$ $\geq$ 600
Demand for Center D | $X_{14}$ + $X_{24}$ + $X_{34}$ $\geq$ 590
Demand for Center E | $X_{15}$ + $X_{25}$ + $X_{35}$ $\geq$ 880
Integer, Non-Negativity Constraints | $X_{11}$ to $X_{35}$ all integers and $\geq$ 0

```{r q5a-lp, echo=TRUE}

# defining parameters 
objective.fn <- c(12.4, 13.75, 12.3, 12.15, 18.35,
                  8.5, 18.55, 8.80, 9.85, 16.45,
                  9.3, 14.25, 6.45, 11.15, 14.95)
const.mat = matrix(c(# Constraint 1: Capacity constraint
                     rep(1,5),rep(0,10),
                     rep(0,5),rep(1,5),rep(0,5),
                     rep(0,10),rep(1,5),
                     # Constraint 2: Demand constraint
                     rep(c(1,0,0,0,0) , 3),
                     rep(c(0,1,0,0,0) , 3),
                     rep(c(0,0,1,0,0) , 3),
                     rep(c(0,0,0,1,0) , 3),
                     rep(c(0,0,0,0,1) , 3)
                     ), ncol = 15, byrow = TRUE)
const.dir = c(rep('<=', 3), rep('>=', 5))
const.rhs = c(1300,900,400,160,330,600,590,880)

# solving the linear problem
lp.solution = lp(direction = 'min', objective.fn, 
              const.mat, const.dir, const.rhs,all.int = T,
              compute.sens = TRUE)
matrix(c(lp.solution$solution), ncol = 5, byrow = T)
print(lp.solution)


```

The optimal solution would be to send 330,50 and 880 from Factory A to Centers B,D and E respectively. Send 160, 200 and 540 tins of milk powder from factory B to centers A, C and D respectively and send 400 tins of milk powder from factory C to center C. This would lead to the minimum transportation cost of $32321.

### Question 5b

``` {r 5b-model 1}
# defining parameters 
objective.fn <- c(12.4, 13.75, 12.3, 12.15, 18.35,
                  8.5, 18.55, 8.80, 9.85, 16.45,
                  9.3, 14.25, 6.45, 11.15, 14.95,
                  11.4,10.6,9.75,13.55,11.95)
const.mat = matrix(c(# Constraint 1: Capacity constraint
                     rep(1,5),rep(0,15),
                     rep(0,5),rep(1,5),rep(0,10),
                     rep(0,10),rep(1,5),rep(0,5),
                     rep(0,15),rep(1,5),
                     # Constraint 2: Demand constraint
                     rep(c(1,0,0,0,0) , 4),
                     rep(c(0,1,0,0,0) , 4),
                     rep(c(0,0,1,0,0) , 4),
                     rep(c(0,0,0,1,0) , 4),
                     rep(c(0,0,0,0,1) , 4)
                     ), ncol = 20, byrow = TRUE)
const.dir = c(rep('<=', 4), rep('>=', 5))
const.rhs = c(1300,900,400,1200,160*1.1,330*1.1,600*1.1,590*1.1,880*1.1)

# solving the linear problem
lp.solution = lp(direction = 'min', objective.fn, 
              const.mat, const.dir, const.rhs,all.int = T,
              compute.sens = TRUE)
matrix(c(lp.solution$solution), ncol = 5, byrow = T)
print(lp.solution)

```

```{r q5b-lp-model 2, echo=TRUE}

# defining parameters 
objective.fn <- c(12.4, 13.75, 12.3, 12.15, 18.35,
                  8.5, 18.55, 8.80, 9.85, 16.45,
                  9.3, 14.25, 6.45, 11.15, 14.95,
                  14.1,15.5,13.85,9.45,12.25)
const.mat = matrix(c(# Constraint 1: Capacity constraint
                     rep(1,5),rep(0,15),
                     rep(0,5),rep(1,5),rep(0,10),
                     rep(0,10),rep(1,5),rep(0,5),
                     rep(0,15),rep(1,5),
                     # Constraint 2: Demand constraint
                     rep(c(1,0,0,0,0) , 4),
                     rep(c(0,1,0,0,0) , 4),
                     rep(c(0,0,1,0,0) , 4),
                     rep(c(0,0,0,1,0) , 4),
                     rep(c(0,0,0,0,1) , 4)
                     ), ncol = 20, byrow = TRUE)
const.dir = c(rep('<=', 4), rep('>=', 5))
const.rhs = c(1300,900,400,1200,160*1.1,330*1.1,600*1.1,590*1.1,880*1.1)

# solving the linear problem
lp.solution = lp(direction = 'min', objective.fn, 
              const.mat, const.dir, const.rhs,all.int = T,
              compute.sens = TRUE)
matrix(c(lp.solution$solution), ncol = 5, byrow = T)
print(lp.solution)

```

From the two linear models, we can see that the minimum transport costs to fulfill the increased demands should Factory D be opened would be $29010.20, while the minimum transport costs to fulfill the increased demands should factory E be opened would be \$29513.10. Hence, factory D should be opened instead of factory E if we want to minimize transportation costs.

### Question 6a : Simple Moving Average & Mean Square Error(MSE)
```{r q6a, echo=TRUE}

week <- c(1,2,3,4,5,6,7,8,9,10)
tins_sold <- c(15,18,37,40,39,38,35,41,55,30)
milk_df <- data.frame(week,tins_sold)
colnames(milk_df) <- c('Week','Tins Sold')
#milk_df$tins_soldSMA <- SMA(milk_df$`Tins Sold`,n=1)
#mean((milk_df$tins_soldSMA - milk_df$`Tins Sold`)^2, na.rm=T)
#find best value for k to give smallest mean squared error
for(k in 1:8) {
  milk_df$tins_soldSMA<- dplyr::lag(SMA(milk_df$`Tins Sold`, n=k),1)
  print(k)
  print(mean((milk_df$tins_soldSMA - milk_df$`Tins Sold`)^2,na.rm=T))
}

# Based on the results, k = 5 gives smallest MSE
milk_df$soldSMA <- dplyr::lag(SMA(milk_df$`Tins Sold`, n=5),1)
milk_df$soldSMA
#View(milk_df)

milk_dfmain <- milk_df[1:7,]
milk_dfHeldout <- milk_df[8:10,]
#View(milk_dfHeldout)

hwcorrect <- HoltWinters(milk_df$`Tins Sold`,gamma=F)
hwcorrect_pred <- predict(hwcorrect,n.ahead = 1 ) 
hwcorrect_pred # gives 49
plot(hwcorrect)

for(k in c(0.1,0.3,0.5,0.7,0.9)) {
  hw1 <- HoltWinters(milk_dfmain$`Tins Sold`,alpha=k,gamma=F)
  hw1_pred <- predict(hw1,n.ahead = 3)
  print(k)
  print(sqrt(mean((hw1_pred-milk_dfHeldout$`Tins Sold`)^2,na.rm = T)))
}

hwAns <- HoltWinters(milk_df$`Tins Sold`,alpha = 0.9, gamma=F)
hwAns_pred <- predict(hwAns,n.ahead = 1 ) # gives 49
plot(hwAns)
hwAns_pred
```


Using the SMA model, we lag the data by 1 so that we can use data from the past k weeks to predict the data for the k+1th week. From there, we find out that from values of [1:8], k=5 gives the smallest mean squared error. This means that we use data from the past 5 weeks to predict the number of milk tins sold for the next week. The forecast for week 11 would thus be the sum of sales from weeks 6-10 divided by 5=40.

We first create two data sets, one for weeks 1-7, and another for weeks 8,9,10 to be used as our test case. Using the data from weeks 1-7, we apply a Holt Winters model with different smoothing constants. We then use this model to predict the sales volume for weeks 8-10. Using this prediction and the actual data from weeks 8-10, we calculate the RMSE and find the smoothing constant which gives the smallest RMSE. We then use this smoothing constant to predict the sales volume for week 11, which would be 49.
